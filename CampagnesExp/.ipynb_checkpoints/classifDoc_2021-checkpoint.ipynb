{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de documents : prise en main des outils\n",
    "\n",
    "Le but de ce TP est de classer des documents textuels... Dans un premier temps, nous allons vérifier le bon fonctionnement des outils sur des données jouets puis appliquer les concepts sur des données réelles.\n",
    "\n",
    "\n",
    "## Conception de la chaine de traitement\n",
    "Pour rappel, une chaine de traitement de documents classique est composée des étapes suivantes:\n",
    "1. Lecture des données et importation\n",
    "    - Dans le cadre de nos TP, nous faisons l'hypothèse que le corpus tient en mémoire... Si ce n'est pas le cas, il faut alors ajouter des structures de données avec des buffers (*data-reader*), bien plus complexes à mettre en place.\n",
    "    - Le plus grand piège concerne l'encodage des données. Dans le TP... Pas (ou peu) de problème. Dans la vraie vie: il faut faire attention à toujours maitriser les formats d'entrée et de sortie.\n",
    "1. Traitement des données brutes paramétrique. Chaque traitement doit être activable ou desactivable + paramétrable si besoin.\n",
    "    - Enlever les informations *inutiles* : chiffre, ponctuations, majuscules, etc... <BR>\n",
    "    **L'utilité dépend de l'application!**\n",
    "    - Segmenter en mots (=*Tokenization*)\n",
    "    - Elimination des stop-words\n",
    "    - Stemming/lemmatisation (racinisation)\n",
    "    - Byte-pair encoding pour trouver les mots composés (e.g. Sorbonne Université, Ville de Paris, Premier Ministre, etc...)\n",
    "1. Traitement des données numériques\n",
    "    - Normalisation *term-frequency* / binarisation\n",
    "    - Normalisation *inverse document frequency*\n",
    "    - Elimination des mots rares, des mots trop fréquents\n",
    "    - Construction de critère de séparabilité pour éliminer des mots etc...\n",
    "1. Apprentissage d'un classifieur\n",
    "    - Choix du type de classifieur\n",
    "    - Réglage des paramètres du classifieur (régularisation, etc...)\n",
    "\n",
    "## Exploitation de la chaine de traitement\n",
    "\n",
    "On appelle cette étape la réalisation d'une campagne d'expériences: c'est le point clé que nous voulons traviller en TAL cette année.\n",
    "1. Il est impossible de tester toutes les combinaisons par rapport aux propositions ci-dessus... Il faut donc en éliminer un certain nombre.\n",
    "    - En discutant avec les experts métiers\n",
    "    - En faisant des tests préliminaires\n",
    "1. Après ce premier filtrage, il faut:\n",
    "    - Choisir une évaluation fiable et pas trop lente (validation croisée, leave-one-out, split apprentissage/test simple)\n",
    "    - Lancer des expériences en grand\n",
    "        - = *grid-search*\n",
    "        - parallèliser sur plusieurs machines\n",
    "        - savoir lancer sur un serveur et se déconnecter\n",
    "1. Collecter et analyser les résultats\n",
    "\n",
    "\n",
    "## Inférence\n",
    "\n",
    "L'inférence est ensuite très classique: la chaine de traitement optimale est apte à traiter de nouveaux documents\n",
    "\n",
    "# Etape 1: charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données:\n",
    "def load_pres(fname):\n",
    "    alltxts = []\n",
    "    alllabs = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        #\n",
    "        lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "        if lab.count('M') >0:\n",
    "            alllabs.append(-1)\n",
    "        else: \n",
    "            alllabs.append(1)\n",
    "        alltxts.append(txt)\n",
    "    return alltxts,alllabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pres_test(fname):\n",
    "    alltxts = []\n",
    "    alllabs = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        #\n",
    "        lab = re.sub(r\"<[0-9]*:[0-9]*(.)>*\",\"\\\\1\",txt)\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*()>*\",\"\\\\1\",txt)\n",
    "        if lab.count('M') >0:\n",
    "            alllabs.append(-1)\n",
    "        else: \n",
    "            alllabs.append(1)\n",
    "        alltxts.append(txt)\n",
    "    return alltxts,alllabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"./AFDpresidentutf8/corpus.tache1.learn.utf8\"\n",
    "\n",
    "alltxtsPresidents,alllabsPresidents = load_pres(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"./AFDpresidentutf8/corpus.tache1.test.utf8\"\n",
    "\n",
    "alltxtsPresidentsTests,_ = load_pres_test(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27162\n",
      " En répondant à votre invitation, en effectuant cette première visite d'Etat d'un Président français en Algérie depuis l'indépendance, j'ai conscience d'ouvrir avec vous un chapitre nouveau de notre histoire commune, le chapitre de la confiance, de l'estime, du respect mutuel, de l'amitié et de la solidarité.\n",
      "\n",
      " Dans cette perspective, je demanderai à une grande personnalité de l'industrie de me faire des propositions sur les moyens de renforcer la coopération scientifique et industrielle avec les pays émergents, les transferts de technologie vers ces pays et le financement de leur développement propre.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(alltxtsPresidentsTests))\n",
    "print(alltxtsPresidentsTests[0])\n",
    "print(alltxtsPresidentsTests[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57413 57413\n",
      " Quand je dis chers amis, il ne s'agit pas là d'une formule diplomatique, mais de l'expression de ce que je ressens.\n",
      "\n",
      "1\n",
      " Je compte sur vous.\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(alltxtsPresidents),len(alllabsPresidents))\n",
    "print(alltxtsPresidents[0])\n",
    "print(alllabsPresidents[0])\n",
    "print(alltxtsPresidents[-1])\n",
    "print(alllabsPresidents[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movies(path2data): # 1 classe par répertoire\n",
    "    alltxts = [] # init vide\n",
    "    labs = []\n",
    "    cpt = 0\n",
    "    for cl in os.listdir(path2data): # parcours des fichiers d'un répertoire\n",
    "        for f in os.listdir(path2data+cl):\n",
    "            txt = open(path2data+cl+'/'+f).read()\n",
    "            alltxts.append(txt)\n",
    "            labs.append(cpt)\n",
    "        cpt+=1 # chg répertoire = cht classe\n",
    "        \n",
    "    return alltxts,labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './movies1000/test/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-5f3a643ba549>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./movies1000/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0malltxtsMovies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malllabsMovies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_movies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-701445d4a262>\u001b[0m in \u001b[0;36mload_movies\u001b[1;34m(path2data)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# parcours des fichiers d'un répertoire\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2data\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2data\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0malltxts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mlabs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './movies1000/test/data'"
     ]
    }
   ],
   "source": [
    "path = \"./movies1000/\"\n",
    "\n",
    "alltxtsMovies,alllabsMovies = load_movies(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentimentsTests(filename):\n",
    "    alltxts = [] # init vide\n",
    "    labs = []\n",
    "    cpt = 0\n",
    "    \n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "        print(lines)\n",
    "    return alltxts,labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 920561: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-7ae54ce43a7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./movies1000/test/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0malltxtsSentimentsTest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malllabsSentimentsTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_movies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-9841b49f4003>\u001b[0m in \u001b[0;36mload_movies\u001b[1;34m(path2data)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# parcours des fichiers d'un répertoire\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2data\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2data\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0malltxts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mlabs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 920561: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "filename = \"./movies1000/test/\"\n",
    "\n",
    "alltxtsSentimentsTest,alllabsSentimentsTest = load_movies(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 196\n",
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n",
      "\n",
      "1\n",
      "\n",
      "#######################################\n",
      "\n",
      "\n",
      " The wife here has a sharp tongue and a strong will, and so Taylor plays her movie star heroine with more spirit than she was given credit for",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(alltxtsSentimentsTest),len(alllabsSentimentsTest))\n",
    "print(alltxtsSentimentsTest[0])\n",
    "print(alllabsSentimentsTest[0])\n",
    "print(\"\\n#######################################\\n\\n\")\n",
    "print(alltxtsSentimentsTest[-1])\n",
    "print(alllabsSentimentsTest[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
      "they seem to have taken this pretty neat concept , but executed it terribly . \n",
      "so what are the problems with the movie ? \n",
      "well , its main problem is that it's simply too jumbled . \n",
      "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what's going on . \n",
      "there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \n",
      "now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem . \n",
      "it's obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \n",
      "and do they make things entertaining , thrilling or even engaging , in the meantime ? \n",
      "not really . \n",
      "the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn't the make the film all that more entertaining . \n",
      "i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \n",
      "i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \n",
      "okay , we get it . . . there \n",
      "are people chasing her and we don't know who they are . \n",
      "do we really need to see it over and over again ? \n",
      "how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \n",
      "apparently , the studio took this film away from its director and chopped it up themselves , and it shows . \n",
      "there might've been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \n",
      "the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \n",
      "but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character's unraveling . \n",
      "overall , the film doesn't stick because it doesn't entertain , it's confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \n",
      "oh , and by the way , this is not a horror or teen slasher flick . . . it's \n",
      "just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "\n",
      "0\n",
      "\n",
      "#######################################\n",
      "\n",
      "\n",
      "truman ( \" true-man \" ) burbank is the perfect name for jim carrey's character in this film . \n",
      "president truman was an unassuming man who became known worldwide , in spite of ( or was it because of ) his stature . \n",
      " \" truman \" also recalls an era of plenty following a grim war , an era when planned communities built by government scientists promised an idyllic life for americans . \n",
      "and burbank , california , brings to mind the tonight show and the home of nbc . \n",
      "if hollywood is the center of the film world , burbank is , or was , the center of tv's world , the world where our protagonist lives . \n",
      "combine all these names and concepts into \" truman burbank , \" and you get something that well describes him and his artificial world . \n",
      "truman leads the perfect life . \n",
      "his town , his car , and his wife are picture perfect . \n",
      "his idea of reality comes under attack one day when a studio light falls from the sky . \n",
      "the radio explains that an overflying airplane started coming apart . \n",
      " . \n",
      " . \n",
      "but then why would an airplane be carrying a studio light ? \n",
      "the next day during the drive to work , the radio jams and he starts picking up a voice that exactly describes his movements . \n",
      "he is so distracted that he nearly hits a pedestrian . \n",
      "when the radio comes back to normal , the announcer warns listeners to drive carefully . \n",
      "his suspicion aroused , he wanders around the town square looking for other oddities . \n",
      "the world appears to be functioning properly until he enters an office building and tries to take the elevator . \n",
      "the elevator doors open up on a small lounge with people on coffee breaks . \n",
      "a grip sees truman him and quickly moves a paneled door , made to look like the back of an elevator , into place . \n",
      "two security guards grab him and throw him out . \n",
      "truman is really suspicious now . \n",
      "it gets even worse the next day when his wife , a nurse , describes an elevator accident in the building where he saw the lounge . \n",
      " \" it's best not to think about it , \" she says , trying vainly to change truman's memory . \n",
      "truman becomes determined to see who or what is behind this apparently elaborate hoax at his expense . \n",
      "at every turn he is stopped by an amazing coincidence that just happens to keep him in his own little town . \n",
      "his last hope is to quell his fear of the ocean and sail to the edge of the world . \n",
      "you know by now that truman's life is the subject of a television program . \n",
      "his actions are \" real \" but everything else is carefully scripted , from the death of his father to the choice of his wife . \n",
      "truman is determined to find out what the big hoax is . \n",
      "meanwhile , christof , the all-seeing creator of truman's world does his best to keep him unaware and happy . \n",
      "it's sort of like westworld told from the robots' point of view , or jurassic park from the dinosaurs' point of view . \n",
      "we root for the captive of the cage-world . \n",
      "our protagonist is counting on \" chaos theory \" to help him escape his elaborate trap . \n",
      "the story , written by andrew niccol ( writer/director of gattaca ) , introduces some interesting questions , such as the ethics of subjecting a person to this type of life , or the psychological impact of learning that your entire life has all been fake . \n",
      "although these questions came to mind , i don't think the film itself asked them . \n",
      "it certainly didn't address them or try to answer them . \n",
      "i was particularly disappointed that the film didn't deal more with the trauma of learning one's life is a tv show . \n",
      "carrey's performance at the end showed a smidgen of truman's pain , but i almost felt that he got over it too easily for the sake of the film's pacing . \n",
      "earlier in the movie i found myself wondering if it would be better for truman to find out the truth or whether i should root for him to be well . \n",
      "the two seemed exclusive of one another , but weir and niccol didn't see it that way . \n",
      "perhaps it's not fair to criticize a movie for what it isn't , but it seems like there were some missed opportunities here . \n",
      "but on its own terms , the movie is well made . \n",
      "sight , sound and pacing are all handled competently . \n",
      "much of the first part of the movie is the truman show . \n",
      "the scenes are all apparently shot from hidden cameras , with snoots and obstructions covering the corners of the screen . \n",
      "one hidden camera is apparently in his car radio , the green led numbers obscuring the lower part of the screen . \n",
      "the music is well-chosen and scored . \n",
      "the film opens with what sounds like family drama theme music , when truman's world is still beautiful and perfect . \n",
      "when the movie ends , the score sounds more like a frantic , driven , tangerine dream opus , while still keeping the same timbre . \n",
      "philip glass' epic music ( from powaqqatsi ) permeates truman's scenes of suspicion and awakening . \n",
      " ( glass has a small cameo as a keyboardist for the show . ) \n",
      "and the pacing of the story was brisk . \n",
      "there was no unnecessarily long setup explaining the concept behind the truman show , just a few quick title cards , a few interviews , and then right into the show , and the movie . \n",
      "one of the first scenes is of the studio light falling ; there was no token scene of truman's idyllic life before it falls apart , because it wasn't necessary , we pick up the story at the first sign of trouble , and no sooner . \n",
      "there's also no point in the movie where the plot slows down . \n",
      "it's a quick , straight shot to the movie's end . \n",
      "in terms of overall quality , i would compare the truman show to niccol's gattaca . \n",
      "both films are well made with interesting stories set in interesting worlds . \n",
      "but neither film really felt like it capitalized on all the great ideas ; neither film \" clicked \" and became an instant classic . \n",
      "nevertheless , i look forward to niccol's next film , whatever it may be . \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(alltxtsMovies),len(alllabsMovies))\n",
    "print(alltxtsMovies[0])\n",
    "print(alllabsMovies[0])\n",
    "print(\"\\n#######################################\\n\\n\")\n",
    "print(alltxtsMovies[-1])\n",
    "print(alllabsMovies[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation paramétrique du texte\n",
    "\n",
    "Vous devez tester, par exemple, les cas suivants:\n",
    "- transformation en minuscule ou pas\n",
    "- suppression de la ponctuation\n",
    "- transformation des mots entièrement en majuscule en marqueurs spécifiques\n",
    "- suppression des chiffres ou pas\n",
    "- conservation d'une partie du texte seulement (seulement la première ligne = titre, seulement la dernière ligne = résumé, ...)\n",
    "- stemming\n",
    "- ...\n",
    "\n",
    "\n",
    "Vérifier systématiquement sur un exemple ou deux le bon fonctionnement des méthodes sur deux documents (au moins un de chaque classe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def transform(text, punc=False,accentMaj=False,nb=False, stopW=False) :\n",
    "    text_transf = text\n",
    "    \n",
    "    for i in range(len(text)) :\n",
    "        \n",
    "        if punc:\n",
    "            punc = string.punctuation  # recupération de la ponctuation\n",
    "            punc += '\\n\\r\\t'\n",
    "            text_transf[i] = text_transf[i].translate(str.maketrans(punc, ' ' * len(punc)))  \n",
    "        if accentMaj:\n",
    "            # suppression des accents et des caractères non normalisés\n",
    "            text_transf[i] = unicodedata.normalize('NFD', text_transf[i]).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "            text_transf[i] = text_transf[i].lower()\n",
    "        \n",
    "        if nb:\n",
    "            # suppression des nombres\n",
    "            text_transf[i] = re.sub('[0-9]+', '', text_transf[i]) # remplacer une séquence de chiffres par rien\n",
    "        if stopW:\n",
    "            for w in stopwords.words('english'):\n",
    "                text_transf[i] = text_transf[i].replace(w, '') \n",
    "    return text_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_transform_movies = transform(alltxtsMovies,punc=True,accentMaj=True,nb=True, stopW=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_transform_presidents = transform(alltxtsPresidents,punc=True,accentMaj=True,nb=True, stopW=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction du vocabulaire\n",
    "\n",
    "Exploration préliminaire des jeux de données.\n",
    "\n",
    "- Quelle est la taille d'origine du vocabulaire?\n",
    "- Que reste-t-il si on ne garde que les 100 mots les plus fréquents? [word cloud]\n",
    "- Quels sont les 100 mots dont la fréquence documentaire est la plus grande? [word cloud]\n",
    "- Quels sont les 100 mots les plus discriminants au sens de odds ratio? [word cloud]\n",
    "- Quelle est la distribution d'apparition des mots (Zipf)\n",
    "- Quels sont les 100 bigrammes/trigrammes les plus fréquents?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mots = []\n",
    "\n",
    "for t in txt_transform_movies:\n",
    "    mots += (t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "dico = Counter(mots)\n",
    "#print(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction() :\n",
    "    \n",
    "    # Taille d'origine du vocabulaire\n",
    "    print(\"taille du voc \", len(dico))\n",
    "    # 100 mots les plus fréquents\n",
    "    print(dico.most_common(100))\n",
    "    # 100 mots à la fréquence documentaire la plus grande\n",
    "    \n",
    "    # 100 mots les plus discriminants au sens de odds ratio\n",
    "    \n",
    "    # Distribution d'apparition des mots\n",
    "    \n",
    "    # Bigrammes et trigrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = dict(zip(list(dico.keys()), np.arange(len(dico)).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38960"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros((len(txt_transform_movies),len(trans)))\n",
    "\n",
    "for ind_txt in range(len(txt_transform_movies)):\n",
    "    for m in txt_transform_movies[ind_txt].split():\n",
    "        d[ind_txt][trans[m]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_transform_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passage à une matrice sparse\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "ds = coo_matrix(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question qui devient de plus en plus intéressante avec les approches modernes:\n",
    "est-il possible d'extraire des tri-grammes de lettres pour représenter nos documents?\n",
    "\n",
    "Quelle performances attendrent? Quels sont les avantages et les inconvénients d'une telle approche?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles de Machine Learning\n",
    "\n",
    "Avant de lancer de grandes expériences, il faut se construire une base de travail solide en étudiant les questions suivantes:\n",
    "\n",
    "- Combien de temps ça prend d'apprendre un classifieur NB/SVM/RegLog sur ces données en fonction de la taille du vocabulaire?\n",
    "- La validation croisée est-elle nécessaire? Est ce qu'on obtient les mêmes résultats avec un simple *split*?\n",
    "- La validation croisée est-elle stable? A partir de combien de fold (travailler avec différentes graines aléatoires et faire des statistiques basiques)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_movies = vectorizer.fit_transform(txt_transform_movies)\n",
    "\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer()\n",
    "X_presidents = vectorizer.fit_transform(txt_transform_presidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57413, 27054)\n",
      "57413\n"
     ]
    }
   ],
   "source": [
    "print(X_presidents.shape)\n",
    "print(len(alllabsPresidents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 38960)\n",
      "(2000, 38890)\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(ds.shape)\n",
    "print(X.shape)\n",
    "print(len(alllabsMovies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nb.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santh\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8953198363934006"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {}\n",
    "# Grid Search CV implementation\n",
    "grid_cv = GridSearchCV(clf, param_grid, n_jobs=-1, cv=3)\n",
    "grid_cv.fit(X_presidents, alllabsPresidents)\n",
    "# Return set of parameters with the best performance\n",
    "grid_cv.best_params_\n",
    "# Return the performance metric score\n",
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8075 0.79   0.7725 0.8325 0.78  ]\n",
      "[0.7525 0.78   0.78   0.7825 0.7675]\n",
      "[0.7525 0.78   0.78   0.7825 0.7675]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# usage en boucle implicite\n",
    "# le classifieur est donné en argument, tout ce fait implicitement (possibilité de paralléliser avec @@n_jobs@@)\n",
    "scores = cross_val_score( clf,ds, alllabs, cv=5)\n",
    "scoresSVC = cross_val_score( svc,ds, alllabs, cv=5)\n",
    "scoreslin = cross_val_score( svc,ds, alllabs, cv=5)\n",
    "\n",
    "print(scores)\n",
    "print(scoresSVC)\n",
    "print(scoreslin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première campagne d'expériences\n",
    "\n",
    "Les techniques sur lesquelles nous travaillons étant sujettes au sur-apprentissage: trouver le paramètre de régularisation dans la documentation et optimiser ce paramètre au sens de la métrique qui vous semble la plus appropriée (cf question précédente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equilibrage des données\n",
    "\n",
    "Un problème reconnu comme dur dans la communauté est celui de l'équilibrage des classes (*balance* en anglais). Que faire si les données sont à 80, 90 ou 99% dans une des classes?\n",
    "Le problème est dur mais fréquent; les solutions sont multiples mais on peut isoler 3 grandes familles de solution.\n",
    "\n",
    "1. Ré-équilibrer le jeu de données: supprimer des données dans la classe majoritaire et/ou sur-échantilloner la classe minoritaire.<BR>\n",
    "   $\\Rightarrow$ A vous de jouer pour cette technique\n",
    "1. Changer la formulation de la fonction de coût pour pénaliser plus les erreurs dans la classe minoritaire:\n",
    "soit une fonction $\\Delta$ mesurant les écarts entre $f(x_i)$ et $y_i$ \n",
    "$$C = \\sum_i  \\alpha_i \\Delta(f(x_i),y_i), \\qquad \\alpha_i = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\mbox{si } y_i \\in \\mbox{classe majoritaire}\\\\\n",
    "B>1 & \\mbox{si } y_i \\in \\mbox{classe minoritaire}\\\\\n",
    "\\end{array} \\right.$$\n",
    "<BR>\n",
    "   $\\Rightarrow$ Les SVM et d'autres approches sklearn possèdent des arguments pour régler $B$ ou $1/B$... Ces arguments sont utiles mais pas toujours suffisant.\n",
    "1. Courbe ROC et modification du biais. Une fois la fonction $\\hat y = f(x)$ apprise, il est possible de la *bidouiller* a posteriori: si toutes les prédictions $\\hat y$ sont dans une classe, on va introduire $b$ dans $\\hat y = f(x) + b$ et le faire varier jusqu'à ce qu'un des points change de classe. On peut ensuite aller de plus en plus loin.\n",
    "Le calcul de l'ensemble des scores associés à cette approche mène directement à la courbe ROC.\n",
    "\n",
    "**Note:** certains classifieurs sont intrinsèquement plus résistante au problème d'équilibrage, c'est par exemple le cas des techniques de gradient boosting que vous verrez l'an prochain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
